<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>手写多层感知机</title>
      <link href="/2022/05/28/%E6%89%8B%E5%86%99%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
      <url>/2022/05/28/%E6%89%8B%E5%86%99%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="激活函数">激活函数</h1><p>为了引入非线性运算，神经网络在每个层之间增加了一个按元素进行非线性运算的操作，也就是激活函数(Activation Function) 常见的激活函数有</p><h2 id="relu函数">ReLU函数</h2><h3 id="数学表达">数学表达</h3><p>ReLU（rectified linear unit）函数提供了一个很简单的非线性变换. <span class="math display">\[\operatorname{ReLU}(x)=\max (x, 0)\]</span> <img src="/2022/05/28/%E6%89%8B%E5%86%99%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/2.png" alt="ReLU"> 其导数为: <span class="math display">\[\left\{\begin{array}{ll}1, &amp; x \geqslant 0 \\0, &amp; x&lt;0\end{array}\right.\]</span> <img src="/2022/05/28/%E6%89%8B%E5%86%99%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/3.png" alt="ReLU导数"></p><h3 id="代码实现">代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> nd</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="keyword">return</span> nd.maximum(X,<span class="number">0</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def relu_prime(y):</span><br><span class="line">    z = nd.copy(y)</span><br><span class="line">    z[y&gt;0] = 1</span><br><span class="line">    z[y&lt;0] = 0</span><br><span class="line">    z[y == 0] =0.5</span><br><span class="line">    return z</span><br></pre></td></tr></table></figure><h2 id="sigmoid">sigmoid</h2><h3 id="数学表达-1">数学表达</h3><p>sigmoid函数可以将元素的值变换到0和1之间： <span class="math display">\[\operatorname{sigmoid}(x)=\frac{1}{1+\exp (-x)}\]</span> <img src="/2022/05/28/%E6%89%8B%E5%86%99%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/4.png" alt="sigmoid"> 其导数为: <span class="math display">\[\operatorname{sigmoid}{ }^{\prime}(x)=\operatorname{sigmoid}(x)(1-\operatorname{sigmoid}(x))\]</span> <img src="/2022/05/28/%E6%89%8B%E5%86%99%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/5.png" alt="sigmoid导数"></p><h3 id="代码实现-1">代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+nd.exp(-X)),X</span><br><span class="line">~~~:</span><br><span class="line">导数</span><br><span class="line">~~~python</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid_prime</span>(<span class="params">y</span>):</span><br><span class="line">    <span class="keyword">return</span> sigmoid(y)*(<span class="number">1</span> - sigmoid(y))</span><br></pre></td></tr></table></figure><hr><h1 id="前向传播反向传播">前向传播、反向传播</h1><h2 id="前项传播">前项传播</h2><p>前项传播(forward propagation或forward pass)是指：按照顺序(从输入层到输出层)计算和存储神经网络中每层的结果。</p><p>对于一个最简单(不含有偏置)的单隐藏层为 <embed src="手写多层感知机/1.png#pic_center"></p><p>我们假设输入是一个特征为<span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^{d}\)</span>的样本输入，<span class="math inline">\(\boldsymbol{W}^{(h)} \in \mathbb{R}^{h \times d}\)</span>，则中间变量<span class="math inline">\(\boldsymbol{z} \in \mathbb{R}^{h}\)</span>可表示为: <span class="math display">\[\boldsymbol{z}=\boldsymbol{W}^{(1)} \boldsymbol{x}\]</span> 将其按元素进行激活函数<span class="math inline">\(\phi\)</span>运算后，将会得到隐藏层变量<span class="math inline">\(\boldsymbol{h}\in \mathbb{R}^{h }\)</span> <span class="math display">\[\boldsymbol{h}=\phi(\boldsymbol{z})\]</span> 假设输出层参数权重只有<span class="math inline">\(\boldsymbol{W}^{(o)} \in \mathbb{R}^{q \times h}\)</span>,则和隐藏层变量进行乘积可以得到输出层变量<span class="math inline">\(\boldsymbol{O}=\in \mathbb{R}^{q}\)</span> <span class="math display">\[\boldsymbol{o}=\boldsymbol{W}^{(o)} \boldsymbol{h}\]</span> 假设损失函数为<span class="math inline">\(\ell\)</span> , 且样本标签为<span class="math inline">\(y\)</span> , 可以计算出单个数据样本的损失项</p><p><span class="math display">\[L=\ell(\boldsymbol{o}, y)\]</span></p><p>在此，我们并不考虑正则项的加入，我们将其<span class="math inline">\(L\)</span>就作为我们有关给定数据样本的目标函数。 ___ 至此，正向传播过程结束</p><p>其代码实现十分简单:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">X,W1,W2,num_inputs</span>):</span><br><span class="line">    <span class="comment"># 将其输入拉成一个一维向量</span></span><br><span class="line">    X = X.reshape((-<span class="number">1</span>,num_inputs))</span><br><span class="line">    Z = nd.dot(X,W1)</span><br><span class="line">    <span class="comment"># 经过激活函数</span></span><br><span class="line">    h = sigmoid(Z)</span><br><span class="line">    o = nd.dot(h,W2)</span><br><span class="line">    <span class="keyword">return</span> o</span><br></pre></td></tr></table></figure><h2 id="反向传播">反向传播</h2><p>反向传播（backward propagation或backpropagation）指的是计算神经网络参数梯度的方法。 主要根据链式法则对目标函数求可学习参数的偏导，之后对其进行参数更新。</p><h3 id="数学推导">数学推导</h3><p>首先，目标函数对于输出层的梯度为<span class="math inline">\(\partial L / \partial \boldsymbol{o} \in \mathbb{R}^{q}\)</span></p><p>其次，计算最靠近输出层的模型参数梯度<span class="math inline">\(\partial L / \partial W^{(o)} \in \mathbb{R}^{q \times h}\)</span></p><p><span class="math display">\[\frac{\partial J}{\partial \boldsymbol{W}^{(o)}}=\frac{\partial L}{\partial \boldsymbol{o}}\cdot \frac{\partial \boldsymbol{o}}{\partial \boldsymbol{W}^{(o)}}=\frac{\partial J}{\partial \boldsymbol{o}} \boldsymbol{h}^{\top}+\lambda \boldsymbol{W}^{(o)}\]</span></p><p>沿着输出层向隐藏层继续进行反向传播，隐藏层的变量梯度<span class="math inline">\(\partial L / \partial \boldsymbol{h} \in \mathbb{R}^{h}\)</span>:</p><p><span class="math display">\[\frac{\partial L}{\partial \boldsymbol{h}}=\frac{\partial L}{\partial \boldsymbol{o}}\cdot \frac{\partial \boldsymbol{o}}{\partial \boldsymbol{h}}=\boldsymbol{W}^{(o)^{\top}} \frac{\partial L}{\partial \boldsymbol{o}}\]</span></p><p>之后，因为激活函数是按照元素运算，所以对于<span class="math inline">\(\partial L / \partial \boldsymbol{z} \in \mathbb{R}^{h}\)</span>的计算，需要使用按照元素乘法:</p><p><span class="math display">\[\frac{\partial L}{\partial \boldsymbol{z}}=\frac{\partial L}{\partial \boldsymbol{h}}\cdot \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}}=\frac{\partial L}{\partial \boldsymbol{h}} \odot \phi^{\prime}(\boldsymbol{z})\]</span></p><p>最后，可以计算出输入层模型参数的梯度<span class="math inline">\(\partial L / \partial W^{(h)} \in \mathbb{R}^{h \times d}\)</span></p><p><span class="math display">\[\frac{\partial L}{\partial \boldsymbol{W}^{(h)}}=\frac{\partial L}{\partial \boldsymbol{z}}\cdot \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}^{(h)}}=\frac{\partial L}{\partial \boldsymbol{z}} \boldsymbol{x}^{\top}\]</span></p><p>以上推导过程在<strong>必要的</strong>时候根据两个输入形状，会将其输入进行转置或者互换位置进行相乘(<strong>就是以偏导的维度为准，根据实际的偏导维度调整上述式子中任意两个乘积的位置或者将其更改为其输入的偏置</strong>)</p><h3 id="代码实现-2">代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backword</span>(<span class="params">self,dLdo,X</span>):</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># h-&gt;o层   </span></span><br><span class="line">    <span class="comment"># 调过</span></span><br><span class="line">    self.W2_grad = nd.dot(self.h.T,dLdo)</span><br><span class="line">    <span class="comment"># 调过</span></span><br><span class="line">    dLdh = nd.dot(dLdo,self.W2.T)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    dLdz = dLdh*sigmoid_prime(self.h)</span><br><span class="line">        </span><br><span class="line">    X = X.reshape((-<span class="number">1</span>,num_inputs))</span><br><span class="line">    <span class="comment"># 改过</span></span><br><span class="line">    self.W1_grad = nd.dot(X.T, dLdz)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>多层感知机-笔记</title>
      <link href="/2022/05/28/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
      <url>/2022/05/28/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="multilayer-perceptionsmlp">Multilayer perceptions(MLP)</h1><h2 id="单层感知机">单层感知机</h2><p>单层感知机又叫做<strong>稠密层 Dense</strong>(<strong>全连接层</strong>，<strong>线性层</strong>) 主要有两个可学习参数，主要有两个可学习参数:<span class="math inline">\(\mathbf{W} \in \mathbb{R}^{m \times n},\mathbf{b} \in \mathbb{R}^{m}\)</span>,输入数据为<span class="math inline">\(\mathbf{x}\in \mathbb{R}^{n}\)</span> 则其输出为:<span class="math inline">\(\mathbf{y}=\mathbf{W} \mathbf{x}+\mathbf{b} \in \mathbb{R}^{m}\)</span> 我们可以将线性回归和Softmax回归总结如下: - 线性回归可以看做是只有1个输出的全连接层 - Softmax回归可以看做是有<span class="math inline">\(\mathbb{m}\)</span>个输出(<span class="math inline">\(\mathbb{m}\)</span>是分类的类别数)进行Softmax运算</p><h2 id="多层感知机">多层感知机</h2><p>因为单层感知机是一个线性运算，所以简单的单个感知机叠加没法处理非线性问题(如&quot;异或&quot;问题)。 为了引入非线性运算，神经网络在每个层之间增加了一个按元素进行非线性运算的操作，也就是激活函数(Activation Function)</p><h3 id="常见的激活函数">常见的激活函数</h3><h4 id="sigmoid">sigmoid</h4><p><span class="math display">\[\operatorname{sigmoid}(x)=\frac{1}{1+\exp (-x)}\]</span></p><h4 id="relu">ReLU</h4><p><span class="math display">\[\operatorname{ReLU}(x)=\max (x,0)\]</span></p><h3 id="mlp">MLP</h3><p>而所谓的多层感知机就是将多个全连接层堆叠，全连接层之间通过激活函数链接。</p><p>常见的单隐藏层和三层隐藏层的网络结构图如图所示(有颜色的是隐藏层)</p><figure><img src="/2022/05/28/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/2.png" alt="网络结构图"><figcaption>网络结构图</figcaption></figure><p>其中隐藏层个数的选择和隐藏层输出维度的选择都是超参数(需要自己设定)。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多层感知机</title>
      <link href="/2022/05/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2022/05/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="neural-networks">Neural Networks</h1><h2 id="背景">背景</h2><h3 id="text-handcrafted-features-rightarrow-text-learned-features"><span class="math inline">\(\text { Handcrafted Features } \rightarrow \text { Learned Features }\)</span></h3><figure><img src="/2022/05/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/1.png" alt="手动提取特征到学习特征"><figcaption>手动提取特征到学习特征</figcaption></figure><p>传统的机器学习通常需要对原始数据进行一些特征工程提取数据特征，在放到机器学习模型中。</p><p>而神经网络(以及所谓的深度学习，深度神经网络)可以看做是将传统的机器学习模型训练流程中的人工提取特征换成一个神经网络来进行特征提取。</p><p>将特征提取和线性(或者softmax)回归一起进行训练，所以相比于人工提取特征来说，数据经过训练后的特征提取器(神经网络)得到的特征更适合线性(或者softmax)回归，所以精度往往更加。</p><p><strong>缺点</strong>:神经网络模型相比于其他传统的及其学习模型来说，对于<strong>数据量</strong>和<strong>计算量</strong>的需求往往大数个<strong>数量级</strong>（1000倍，1w倍甚至10w倍），主要因素在于计算机在进行特征提取的时候没有人这么多的先验知识,所以只能通过大量的数据不断地去做损失迭代更新来学习。 ____</p><h2 id="neural-networks中常见的特征提取模型">Neural Networks中常见的特征提取模型</h2><ul><li>Multilayer perceptions(MLP)</li><li>Convolutional neural networks(CNN)</li><li>Recurrent neural networks(RNN)</li><li>Transsformers(最近兴起的)</li><li>Attention mechanism</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dimensionality reduction</title>
      <link href="/2022/05/23/Dimensionality-reduction/"/>
      <url>/2022/05/23/Dimensionality-reduction/</url>
      
        <content type="html"><![CDATA[<h1 id="降维dimensionality-reduction">降维(Dimensionality reduction)</h1><h2 id="背景">背景</h2><p>在机器学习中，相比于<strong>训练误差</strong>，更加关心<strong>泛化误差</strong>。</p><p>为了解决<strong>过拟合</strong>问题，大致有以下方法 * 扩大数据集 * 正则化hexo * 降维</p><p>而在平常时维度过高可能会造成<strong>维度灾难</strong>，而在没有扩大数据集的办法的 情况下可以选择将其降维来解决。</p><h3 id="维度灾难disadvantages">维度灾难(disadvantages)</h3><h1 id="预备知识">预备知识</h1><p><span class="math display">\[\text{Data:}x=(x_{1},x_{2},...,x_{n})^\top_{n\times P}=\left[\begin{matrix}x^\top_{1} \\x^\top_{2} \\...\\x^\top_{n}\end{matrix}\right]=\left[\begin{matrix} x_{11} &amp; x_{12} &amp; ... &amp; x_{1p} \\ x_{21} &amp; x_{22} &amp; ... &amp; x_{2p}\\ ... &amp; ... &amp; ... &amp; ... \\ x_{n1} &amp; x_{n2} &amp; ... &amp;x_{np}\end{matrix}\right]_{n\times p}\]</span></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>123</title>
      <link href="/2022/05/23/123/"/>
      <url>/2022/05/23/123/</url>
      
        <content type="html"><![CDATA[<p><span class="math display">\[b\]</span></p><p>da</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>小批量随机梯度下降</title>
      <link href="/2022/05/23/%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
      <url>/2022/05/23/%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</url>
      
        <content type="html"><![CDATA[<h1 id="mini-batch-stochastic-gradient-descentsgd">Mini-batch Stochastic gradient descent(SGD)</h1><h2 id="适用范围">适用范围</h2><p>小批量随机梯度下降是整个深度学习里面，目前来说几乎是唯一求解的办法</p><p>在机器学习里面可以解决除开决策树之外所有模型的求解(例如SVM,Logistic Regression)</p><h2 id="算法流程">算法流程</h2><hr><ul><li><span class="math inline">\(\text {Train by mini-batch SGD (by various other ways as well)}\)</span><ul><li><span class="math inline">\(\text { w model param, } b \text { batch size, } \eta_{t} \text { learning rate at time } t\)</span></li><li><span class="math inline">\(\text { Randomly initialize } \mathbf{w}_{1}\)</span></li><li><span class="math inline">\(\text { Repeat } t=1,2, \ldots \text { until converge }\)</span><ul><li><span class="math inline">\(Randomly samples I_{t} \subset\{1, \ldots, n\} with \left|I_{t}\right|=b\)</span></li><li><span class="math inline">\(\text {Update} \mathbf{w}_{t+1}=\mathbf{w}_{t}-\eta_{t} \nabla_{\mathbf{w}_{t}} \ell\left(\mathbf{X}_{I_{t}}, \mathbf{y}_{I_{t}}, \mathbf{w}_{t}\right)\)</span></li></ul></li><li><span class="math inline">\(\text { Pros: solve all objectives in this course except for trees }\)</span></li><li><span class="math inline">\(\text { Cons: sensitive to hyper-parameters } b \text { and } \eta_{t}\)</span></li></ul></li></ul><hr><ul><li><span class="math inline">\(\mathbf{w} \text { 是模型的参数，包括线性模型的 } \mathrm{w} \text { (权重)和 } \mathrm{b} \text { (偏移) }\)</span></li><li><span class="math inline">\(\mathbf{b} \text { 表示批量大小}\)</span></li><li><span class="math inline">\(\eta_{t} \text { 表示在t时刻的学习率}\)</span><ul><li><span class="math inline">\(\text{在1时刻(初始时刻)随机初始化}\mathbf{w}_{1}\)</span></li><li><span class="math inline">\(\text{持续迭代} t=1,2, \ldots\text{直至模型收敛(收敛条件:)}\)</span><ul><li><span class="math inline">\(\text{每次迭代从样本中随机选取} I_{t} \subset\{1, \ldots, n\}个样本，选取样本数目为\left|I_{t}\right|=b(批量大小)\)</span></li><li><span class="math inline">\(\text{更新}\mathbf{w},进入下一次迭代的\mathbf{w}为: \mathbf{w}_{t+1}=\mathbf{w}_{t}-\eta_{t} \nabla_{\mathbf{w}_{t}} \ell\left(\mathbf{X}_{I_{t}}, \mathbf{y}_{I_{t}}, \mathbf{w}_{t}\right)\)</span></li></ul></li></ul></li><li><span class="math inline">\(\text{优点：小批量随机梯度下降可以解决出决策树以外的模型}\)</span></li><li><span class="math inline">\(\text{缺点：超参数}\mathbf{b}和\eta需要自己设置(难调)\)</span> ___</li></ul><h2 id="实现代码">实现代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, nd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="comment"># 生成数据集</span></span><br><span class="line">num_inputs = <span class="number">2</span>   <span class="comment">#数据维度</span></span><br><span class="line">num_examples = <span class="number">1000</span>   <span class="comment"># 样本个数</span></span><br><span class="line">true_w = [<span class="number">2</span>, -<span class="number">3.4</span>]  <span class="comment"># 权重初始值</span></span><br><span class="line">true_b = <span class="number">4.2</span>  <span class="comment"># 偏置初始值</span></span><br><span class="line">features = nd.random.normal(scale = <span class="number">1</span>,shape=(num_examples, num_inputs))</span><br><span class="line">labels = true_w[<span class="number">0</span>]*features[:,<span class="number">0</span>]+true_w[<span class="number">1</span>]*features[:,<span class="number">1</span>]+true_b</span><br><span class="line">labels += nd.random.normal(loc = <span class="number">0</span>,scale = <span class="number">0.01</span>, shape=labels.shape) </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    random.shuffle(indices) <span class="comment"># 打乱标号</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        <span class="comment">#从i到min(i+i+batch_size(一个批量), num_exmaples(不够整分就直接拿去全部))</span></span><br><span class="line">        j = nd.array(indices[i : <span class="built_in">min</span>(i+batch_size, num_examples) ] ) </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">yield</span> features.take(j), labels.take(j) <span class="comment"># take根据索引返回对应元素</span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line">w = nd.random.normal(scale = <span class="number">0.01</span>, shape=(num_inputs,<span class="number">1</span>))<span class="comment"># num_inputs(输出维度),1的维度</span></span><br><span class="line">b = nd.zeros(shape = (<span class="number">1</span>,))<span class="comment"># 1,表示张量</span></span><br><span class="line"><span class="comment"># 开辟梯度空间</span></span><br><span class="line">w.attach_grad()</span><br><span class="line">b.attach_grad()</span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">return</span> (y_hat-y.reshape(y_hat.shape))**<span class="number">2</span>/<span class="number">2</span></span><br><span class="line"><span class="comment"># 优化方法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>):</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param[:] = param - lr*param.grad/batch_size</span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            l = loss(net(X,w,b),y)</span><br><span class="line">        l.backward()</span><br><span class="line">        sgd([w,b],lr,batch_size)</span><br><span class="line">    train_l = loss(net(features, w, b), labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %f&#x27;</span>%(epoch+<span class="number">1</span>,train_l.mean().asnumpy()))</span><br></pre></td></tr></table></figure><h2 id="参考">参考:</h2><ul><li>https://www.bilibili.com/video/BV1234y1m75j</li><li>https://www.bilibili.com/read/cv13723919?from=note</li><li>https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习优化方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/05/17/hello-world/"/>
      <url>/2022/05/17/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
