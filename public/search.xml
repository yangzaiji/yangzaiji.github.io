<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>机器学习实战——读书笔记1</title>
      <link href="/2022/10/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B01/"/>
      <url>/2022/10/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B01/</url>
      
        <content type="html"><![CDATA[<p>https://archive.ics.uci.edu/ml/machine-learning-databases/breast.cancer-wisconsin/breast-cancer-wisconsin.data</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>公式推导-练习</title>
      <link href="/2022/09/30/%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC-%E7%BB%83%E4%B9%A0/"/>
      <url>/2022/09/30/%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC-%E7%BB%83%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="第一问">第一问</h1><p>对于第一问来说，a c组与b d组的求解过程是对称的，只是对于系数符号有所变换，只需求解a d，b c只需要在所求解的结果上更改分母便可</p><p><span class="math display">\[\begin{equation*}\begin{aligned}e&amp;=m_{1}a+m_{2}b \\f&amp;=m_{3}a+m_{4}b\\g&amp; = m_{1}c+m_{2}d\\h&amp;=m_{3}c+m_{4}d\\\\a&amp;= n_{1}e+n_{2}f\\b&amp;= n_{3}e+n_{4}f\\c&amp;= n_{1}g+n_{2}h\\d&amp;= n_{3}g+n_{4}h\\\end{aligned}\end{equation*}\]</span> 由上式子可以推出 <span class="math display">\[\begin{equation*}\begin{aligned}\\a &amp;= (m_{2}n_{1} + m_{4}n_{2})b/(1 -m_{1}n_{1} - m_{3}n_{2})\\b&amp;=(m_{1}n_{3} + m_{3}n_{4})a/(1 - m_{4}n_{4} - m_{2}n_{3})\\c &amp;= (m_{2}n_{1} + m_{4}n_{2})d/(1 -m_{1}n_{1} - m_{3}n_{2})\\d&amp;=(m_{1}n_{3} + m_{3}n_{4})c/(1 - m_{4}n_{4} - m_{2}n_{3})\\\\\end{aligned}\end{equation*} \]</span> 将其带入<span class="math inline">\(z_{1}=ie+jf\)</span>、<span class="math inline">\(z_{2}=kg+lh\)</span>中 <span class="math display">\[\begin{equation*}\begin{aligned}z_{1} &amp;= ie+jf\\&amp;=i(m_{1}a+m_{2}b)+j(m_{3}a+m_{4}b)\\&amp;=i(m_{1}a+m_{2}(m_{1}n_{3} + m_{3}n_{4})a/(1 - m_{4}n_{4} - m_{2}n_{3}))+j(m_{3}a+m_{4}(m_{1}n_{3} + m_{3}n_{4})a/(1 - m_{4}n_{4} - m_{2}n_{3}))\\\\\\z_{2}&amp;=kg+lh\\&amp;=k(m_{1}c+m_{2}d)+l(m_{3}c+m_{4}d)\\&amp;=k*(m_{1}*(m_{2}*n_{1} + m_{4}*n_{2})*d/(1 -m_{1}*n_{1} - m_{3}*n_{2})+m_{2}*d)+l*(m_{3}*(m_{2}*n_{1} + m_{4}*n_{2})*d/(1 -m_{1}*n_{1} - m_{3}*n_{2})+m_{4}*d)\end{aligned}\end{equation*}\]</span> 可解出<span class="math inline">\(a\)</span>,<span class="math inline">\(d\)</span>: <span class="math display">\[\begin{equation*}\begin{aligned}a&amp;= z1/(i(m_{1} - (m_{2}(m_{1}n_{3} + m_{3}n_{4}))/(m_{2}n_{3} + m_{4}n_{4} - 1)) + j(m_{3} - (m_{4}(m_{1}n_{3} + m_{3}n_{4}))/(m_{2}n_{3} + m_{4}n_{4} - 1)))\\d&amp;=z2/(k(m_{2} - (m_{1}(m_{2}n_{1} + m_{4}n_{2}))/(m_{1}n_{1} + m_{3}n_{2} - 1)) + l(m_{4} - (m_{3}(m_{2}n_{1} + m_{4}n_{2}))/(m_{1}n_{1} + m_{3}n_{2} - 1)))\\\end{aligned}\end{equation*}\]</span></p><h1 id="第二问">第二问</h1><p><span class="math display">\[\begin{equation*}\begin{aligned}z^{新}_{1}&amp;=ie+jg\\&amp;=i(m_{1}a+m_{2}b)+j(m_{1}c+m_{2}d)\\&amp;=i(m_{1}a+m_{2}(m_{1}n_{3} + m_{3}n_{4})a/(1 - m_{4}n_{4} - m_{2}n_{3}))+j(m_{1}c+m_{2}(m_{1}n_{3} + m_{3}n_{4})c/(1 - m_{4}n_{4} - m_{2}n_{3}))\\&amp;= i(m_{1} - (m_{2}(m_{1}n_{3} + m_{3}n_{4}))/(m_{2}n_{3} + m_{4}n_{4} - 1))a + j(m_{1} - (m_{2}(m_{1}n_{3} + m_{3}n_{4}))/(m_{2}n_{3} + m_{4}n_{4} - 1))c\\&amp;= (m_{1} - (m_{2}(m_{1}n_{3} + m_{3}n_{4}))/(m_{2}n_{3} + m_{4}n_{4} - 1))(ia+jc)\\\\z^{新}_{2}&amp;=kf+lh\\&amp;=k(m_{3}a+m_{4}b)+l(m_{3}c+m_{4}d)\\&amp;=k(m_{3}(m_{2}n_{1} + m_{4}n_{2})b/(1 -m_{1}n_{1} - m_{3}n_{2})+m_{4}b)+l(m_{3}(m_{2}n_{1} + m_{4}n_{2})d/(1 -m_{1}n_{1} - m_{3}n_{2})+m_{4}d)\\&amp;=(m_{4} - (m_{3}(m_{2}n_{1} + m_{4}n_{2}))/(m_{1}n_{1} + m_{3}n_{2} - 1))(kb+ld)\end{aligned}\end{equation*} \]</span></p><p><span class="math display">\[\begin{equation*}\begin{aligned}ia+jc&amp;=z^{新}_{1}/(m_{1} - (m_{2}(m_{1}n_{3} + m_{3}n_{4}))/(m_{2}n_{3} + m_{4}n_{4} - 1))\\kf+lh &amp;= z^{新}_{2}/(m_{4} - (m_{3}(m_{2}n_{1} + m_{4}n_{2}))/(m_{1}n_{1} + m_{3}n_{2} - 1))\\\end{aligned}\end{equation*} \]</span></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>自编码神经网络学习笔记</title>
      <link href="/2022/09/29/%E8%87%AA%E7%BC%96%E7%A0%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2022/09/29/%E8%87%AA%E7%BC%96%E7%A0%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="自编码神经网络">自编码神经网络</h1><h2 id="前言">前言</h2><p>神经网络最大的卖点就是其端到端(end-to-end)的过程</p><p>无论是用于图像处理的CNN，还是用于自然语言处理的RNN都可以看做为一种类似编码解码的过程</p><h3 id="从编码-解码角度看cnn">从编码-解码角度看CNN</h3><figure><img src="/2022/09/29/%E8%87%AA%E7%BC%96%E7%A0%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/CNN.png" alt="CNN"><figcaption>CNN</figcaption></figure><p>在CNN中可以将编码-解码看做:</p><p><strong>编码器</strong>:将输入编程成中间表达形式(特征)</p><p><strong>解码器</strong>:将中间表示解码成输出</p><p>例如你将一只猫的图片当做模型的输入，首先图片将会经过一系列的隐藏层进行特征提取。</p><p>该过程就可以看做是一个编码的过程，及将一个原始的东西编码成为一个中间表达形式，其中间表达形式能够更加方便的使得机器进行学习。</p><p>经过一些列的特征提取后，将提取到的特征通过softmax回归进行识别。</p><p>该过程便是将前面隐藏层产生的中间形式进行解码输出。</p><h3 id="从编码-解码角度看rnn">从编码-解码角度看RNN</h3><figure><img src="/2022/09/29/%E8%87%AA%E7%BC%96%E7%A0%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/RNN.png" alt="RNN"><figcaption>RNN</figcaption></figure><p>在RNN中可以将编码-解码看成:</p><p><strong>编码器</strong>:将文本表示成向量</p><p><strong>解码器</strong>:向量表示成输出</p><h2 id="自编码神经网络-1">自编码神经网络</h2><h3 id="编码器-解码器架构">编码器-解码器架构</h3><p>编码器-解码器架构便是将一个模型如图分为两块 1. Encoder 2. Decoder <img src="/2022/09/29/%E8%87%AA%E7%BC%96%E7%A0%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/incode-decode.png" alt="编码器-解码器架构"></p><p>输入经过编码器处理后将其转变成一种中间状态(State)后经过解码器将其转变为输出</p><p>其中经过编码器处理后的中间状态将有助于机器进行学习(因为进行了筛选，等于只把重要信息保留，所以更有助于学习，就如PCA主成分分析的降维功效)</p><h3 id="自编码神经网络-2">自编码神经网络</h3><p>而所谓的自编码神经网络其实就是通过从原始数据中不断的提取重要信息，再利用得到的特征信息重新构建新的数据，而构建的新数据中包含了原始数据中重要的特征信息。其中中间状态是原始数据中的精髓</p><p>可以看出自编码神经网络重头到尾只用到了原始数据的输入信息，并没有用到原始数据的标签，所以自编码神经网络是一个无监督学习的模型</p><figure><img src="/2022/09/29/%E8%87%AA%E7%BC%96%E7%A0%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/AENN.png" alt="自编码神经网络模型结构图"><figcaption>自编码神经网络模型结构图</figcaption></figure><h3 id="训练过程">训练过程</h3><p>其本质上任然是神经网络，并且没有创造出任何特殊层，就只是模型结构的变化</p><p>因此训练过程任然按照神经网络的过程进行训练</p><h2 id="参考">参考</h2><pre><code>1. https://blog.csdn.net/hxxjxw/article/details/106869571?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=%E8%87%AA%E7%BC%96%E7%A0%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-3-106869571.142^v51^new_blog_pos_by_title,201^v3^add_ask&amp;spm=1018.2226.3001.41872. https://www.bilibili.com/video/BV1c54y1E7YP/?spm_id_from=333.999.0.0&amp;vd_source=bba5777f1f15698fba3ee38f0106b6e13. https://www.bilibili.com/video/BV1Vx411j78H/?spm_id_from=333.999.0.0&amp;vd_source=bba5777f1f15698fba3ee38f0106b6e14. https://zh.d2l.ai/chapter_recurrent-modern/encoder-decoder.html </code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>RCNN笔记</title>
      <link href="/2022/08/10/RCNN%E7%AC%94%E8%AE%B0/"/>
      <url>/2022/08/10/RCNN%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="区域卷积神经网络">区域卷积神经网络</h1><h1 id="r-cnn基于区域的卷积神经网络">R-CNN（基于区域的卷积神经网络）</h1><ul><li>最早的检测模型</li></ul><figure><img src="/2022/08/10/RCNN%E7%AC%94%E8%AE%B0/1.png" alt="模型结构图"><figcaption>模型结构图</figcaption></figure><p>模型步骤:</p><ul><li><p>通过Selective search算法(一种启发式算法)来选择多个高质量的提议区域。将其获得的区域截成图片</p></li><li><p>通过一个预训练好的模型(图中两侧的CNN)来对其截取的图片进行特征提取</p></li><li><p>训练一个SVM对类别进行分类</p></li><li><p>训练一个线性模型来预测边缘框的偏移</p></li></ul><h2 id="由于选择的锚框的大小是不同的在这种情况下如何使这些大小不一的锚框变成一个batch">由于选择的锚框的大小是不同的，在这种情况下，如何使这些大小不一的锚框变成一个batch</h2><h2 id="roi-pooling兴趣区域池化层">RoI pooling（兴趣区域池化层）</h2><p><strong>R-CNN 中比较关键的层，作用是将大小不一的锚框变成统一的形状</strong></p><ul><li><p>给定一个锚框，先将其均匀地分割成 n * m 块，然后输出每块里的最大值，</p></li><li><p>这样的话，不管输入进来的锚框有多大，只要给定了 n 和 m 的值，总是输出 n<span class="math inline">\(\times\)</span>m 个</p></li><li><p>这样的话，不同大小的锚框就都可以变成同样的大小，然后就可以作为一个小批量</p></li></ul><figure><img src="/2022/08/10/RCNN%E7%AC%94%E8%AE%B0/2.png" alt="结构图"><figcaption>结构图</figcaption></figure><ul><li><p>左边输入框进入2 <span class="math inline">\(\times\)</span> 2 ROL池化层后</p></li><li><p>先均匀的分成 4块</p></li><li><p>左上 5 = max(0,1,4,5) 右上 6 = max(1,2,5,6)依次类推</p></li></ul><h2 id="兴趣区域池化层roi-pooing与一般的池化层的区别">兴趣区域池化层（RoI Pooing）与一般的池化层的区别</h2><ul><li><p>在一般的池化层中，通过设置池化窗口、填充和步幅的大小来间接控制输出形状</p></li><li><p>在兴趣区域池化层中，对每个区域的输出形状是可以直接指定的</p></li></ul><h2 id="r-cnn">R-CNN</h2><ul><li><p>R-CNN 模型通过预训练的卷积神经网络有效地抽取了图像特征</p></li><li><p>但是速度非常慢（如果从一张图片中选取了上千个提议区域，就需要上千次的卷积神经网络的前向传播来执行目标检测，计算量非常大）</p></li></ul><h1 id="fast-rcnn">Fast RCNN</h1><p>由于RCNN如果从一张图片中选取了上千个区域，就需要上千次的CNN提取特征，计算量非常大</p><p>其主要原因是在于</p><ul><li>R-CNN 对于每个提议区域，卷积神经网络的前向传播是独立的，没有共享计算（这些提议区域通常有重叠，独立的特征提取会导致重复计算）</li></ul><h2 id="fast-rcnn的改进">Fast RCNN的改进</h2><ul><li><p>首先对输入的一整张图片用CNN抽取特征</p></li><li><p>在用RCNN中的启发式搜索 锚框</p></li><li><p>搜索到原始图片上的锚框之后将其（按照一定的比例）映射到 CNN 的输出上</p></li><li><p>映射完锚框之后，再使用 RoI pooling 对 CNN 输出的 feature map 上的锚框进行特征抽取，生成固定长度的特征，之后再通过一个全连接层（这样就不需要使用SVM一个一个的操作，而是一次性操作了）对每个锚框进行预测：物体的类别和真实的边缘框的偏移</p></li></ul><figure><img src="/2022/08/10/RCNN%E7%AC%94%E8%AE%B0/3.png" alt="模型结构图"><figcaption>模型结构图</figcaption></figure><h2 id="总结">总结</h2><h3 id="fast-rcnn对于rcnn的改进">Fast RCNN对于RCNN的改进：</h3><ul><li><p>Fast R-CNN 中的 CNN 不再对每个锚框抽取特征，而是对整个图片进行特征的提取</p></li><li><p>使得锚框出现重复区域不需要重复获取特征</p></li><li><p>然后再在整张图片的feature中找出原图中锚框对应的特征，最后一起做预测</p></li></ul><h1 id="faster-rcnn">Faster RCNN</h1><ul><li><p>为了精确地检测目标结果，Fast R-CNN 模型通常需要在选择性搜索中生成大量的提议区域</p></li><li><p>因此，Faster R-CNN 提出将选择性搜索替换为区域提议网络（region proposal network，RPN），模型的其余部分保持不变，从而减少区域的生成数量，并保证目标检测的精度</p></li></ul><figure><img src="/2022/08/10/RCNN%E7%AC%94%E8%AE%B0/4.png" alt="模型结构图"><figcaption>模型结构图</figcaption></figure><h2 id="faster-rcnn模型">Faster RCNN模型</h2><ul><li><p>使用 RPN 神经网络来替代 selective search</p></li><li><p>RPN 的输入是 CNN 输出的 feature map，输出是一些比较高质量的锚框（可以理解为一个比较小而且比较粗糙的目标检测算法： CNN 的输出进入到 RPN 之后再做一次卷积，然后生成一些锚框（可以是 selective search 或者其他方法来生成初始的锚框）</p></li><li><p>再训练一个二分类问题：预测锚框是否框住了真实的物体以及锚框到真实的边缘框的偏移，最后使用 NMS 进行去重，使得锚框的数量变少</p></li></ul><h2 id="faster-rcnn总结">Faster RCNN总结</h2><ul><li><p>RPN 的作用是生成大量结果很差的锚框，然后进行预测，最终输出比较好的锚框供后面的网络使用（预测出来的比较好的锚框会进入 RoI pooling，后面的操作与 Fast R-CNN 类似)</p></li><li><p>Faster R-CNN 目前来说是用的比较多的算法，准确率比较高，但是速度比较慢</p></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>pytorch——1</title>
      <link href="/2022/08/09/pytorch%E2%80%94%E2%80%941/"/>
      <url>/2022/08/09/pytorch%E2%80%94%E2%80%941/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
        <tags>
            
            <tag> pytorch学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>手写多层感知机</title>
      <link href="/2022/05/28/%E6%89%8B%E5%86%99%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
      <url>/2022/05/28/%E6%89%8B%E5%86%99%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="激活函数">激活函数</h1><p>为了引入非线性运算，神经网络在每个层之间增加了一个按元素进行非线性运算的操作，也就是激活函数(Activation Function) 常见的激活函数有</p><h2 id="relu函数">ReLU函数</h2><h3 id="数学表达">数学表达</h3><p>ReLU（rectified linear unit）函数提供了一个很简单的非线性变换. <span class="math display">\[\operatorname{ReLU}(x)=\max (x, 0)\]</span> <img src="/2022/05/28/%E6%89%8B%E5%86%99%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/2.png" alt="ReLU"> 其导数为: <span class="math display">\[\left\{\begin{array}{ll}1, &amp; x \geqslant 0 \\0, &amp; x&lt;0\end{array}\right.\]</span> <img src="/2022/05/28/%E6%89%8B%E5%86%99%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/3.png" alt="ReLU导数"></p><h3 id="代码实现">代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> nd</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="keyword">return</span> nd.maximum(X,<span class="number">0</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def relu_prime(y):</span><br><span class="line">    z = nd.copy(y)</span><br><span class="line">    z[y&gt;0] = 1</span><br><span class="line">    z[y&lt;0] = 0</span><br><span class="line">    z[y == 0] = 1</span><br><span class="line">    return z</span><br></pre></td></tr></table></figure><h2 id="sigmoid">sigmoid</h2><h3 id="数学表达-1">数学表达</h3><p>sigmoid函数可以将元素的值变换到0和1之间： <span class="math display">\[\operatorname{sigmoid}(x)=\frac{1}{1+\exp (-x)}\]</span> <img src="/2022/05/28/%E6%89%8B%E5%86%99%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/4.png" alt="sigmoid"> 其导数为: <span class="math display">\[\operatorname{sigmoid}{ }^{\prime}(x)=\operatorname{sigmoid}(x)(1-\operatorname{sigmoid}(x))\]</span> <img src="/2022/05/28/%E6%89%8B%E5%86%99%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/5.png" alt="sigmoid导数"></p><h3 id="代码实现-1">代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+nd.exp(-X))</span><br></pre></td></tr></table></figure><p>导数: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid_prime</span>(<span class="params">y</span>):</span><br><span class="line">    <span class="keyword">return</span> sigmoid(y)*(<span class="number">1</span> - sigmoid(y))</span><br></pre></td></tr></table></figure></p><hr><h1 id="前向传播反向传播">前向传播、反向传播</h1><h2 id="前项传播">前项传播</h2><p>前项传播(forward propagation或forward pass)是指：按照顺序(从输入层到输出层)计算和存储神经网络中每层的结果。</p><p>对于一个最简单(不含有偏置)的单隐藏层为 <embed src="手写多层感知机/1.png#pic_center"></p><p>我们假设输入是一个特征为<span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^{d}\)</span>的样本输入，<span class="math inline">\(\boldsymbol{W}^{(h)} \in \mathbb{R}^{h \times d}\)</span>，则中间变量<span class="math inline">\(\boldsymbol{z} \in \mathbb{R}^{h}\)</span>可表示为: <span class="math display">\[\boldsymbol{z}=\boldsymbol{W}^{(1)} \boldsymbol{x}\]</span> 将其按元素进行激活函数<span class="math inline">\(\phi\)</span>运算后，将会得到隐藏层变量<span class="math inline">\(\boldsymbol{h}\in \mathbb{R}^{h }\)</span> <span class="math display">\[\boldsymbol{h}=\phi(\boldsymbol{z})\]</span> 假设输出层参数权重只有<span class="math inline">\(\boldsymbol{W}^{(o)} \in \mathbb{R}^{q \times h}\)</span>,则和隐藏层变量进行乘积可以得到输出层变量<span class="math inline">\(\boldsymbol{O}=\in \mathbb{R}^{q}\)</span> <span class="math display">\[\boldsymbol{o}=\boldsymbol{W}^{(o)} \boldsymbol{h}\]</span> 假设损失函数为<span class="math inline">\(\ell\)</span> , 且样本标签为<span class="math inline">\(y\)</span> , 可以计算出单个数据样本的损失项</p><p><span class="math display">\[L=\ell(\boldsymbol{o}, y)\]</span></p><p>在此，我们并不考虑正则项的加入，我们将其<span class="math inline">\(L\)</span>就作为我们有关给定数据样本的目标函数。 ___ 至此，正向传播过程结束</p><p>其代码实现十分简单:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">X,W1,W2,num_inputs</span>):</span><br><span class="line">    <span class="comment"># 将其输入拉成一个一维向量</span></span><br><span class="line">    X = X.reshape((-<span class="number">1</span>,num_inputs))</span><br><span class="line">    Z = nd.dot(X,W1)</span><br><span class="line">    <span class="comment"># 经过激活函数</span></span><br><span class="line">    h = sigmoid(Z)</span><br><span class="line">    o = nd.dot(h,W2)</span><br><span class="line">    <span class="keyword">return</span> o</span><br></pre></td></tr></table></figure><h2 id="反向传播">反向传播</h2><p>反向传播（backward propagation或backpropagation）指的是计算神经网络参数梯度的方法。 主要根据链式法则对目标函数求可学习参数的偏导，之后对其进行参数更新。</p><h3 id="数学推导">数学推导</h3><p>首先，目标函数对于输出层的梯度为<span class="math inline">\(\partial L / \partial \boldsymbol{o} \in \mathbb{R}^{q}\)</span></p><p>其次，计算最靠近输出层的模型参数梯度<span class="math inline">\(\partial L / \partial W^{(o)} \in \mathbb{R}^{q \times h}\)</span></p><p><span class="math display">\[\frac{\partial L}{\partial \boldsymbol{W}^{(o)}}=\frac{\partial L}{\partial \boldsymbol{o}}\cdot \frac{\partial \boldsymbol{o}}{\partial \boldsymbol{W}^{(o)}}=\frac{\partial L}{\partial \boldsymbol{o}} \boldsymbol{h}^{\top}\]</span></p><p>沿着输出层向隐藏层继续进行反向传播，隐藏层的变量梯度<span class="math inline">\(\partial L / \partial \boldsymbol{h} \in \mathbb{R}^{h}\)</span>:</p><p><span class="math display">\[\frac{\partial L}{\partial \boldsymbol{h}}=\frac{\partial L}{\partial \boldsymbol{o}}\cdot \frac{\partial \boldsymbol{o}}{\partial \boldsymbol{h}}=\boldsymbol{W}^{(o)^{\top}} \frac{\partial L}{\partial \boldsymbol{o}}\]</span></p><p>之后，因为激活函数是按照元素运算，所以对于<span class="math inline">\(\partial L / \partial \boldsymbol{z} \in \mathbb{R}^{h}\)</span>的计算，需要使用按照元素乘法:</p><p><span class="math display">\[\frac{\partial L}{\partial \boldsymbol{z}}=\frac{\partial L}{\partial \boldsymbol{h}}\cdot \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}}=\frac{\partial L}{\partial \boldsymbol{h}} \odot \phi^{\prime}(\boldsymbol{z})\]</span></p><p>最后，可以计算出输入层模型参数的梯度<span class="math inline">\(\partial L / \partial W^{(h)} \in \mathbb{R}^{h \times d}\)</span></p><p><span class="math display">\[\frac{\partial L}{\partial \boldsymbol{W}^{(h)}}=\frac{\partial L}{\partial \boldsymbol{z}}\cdot \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}^{(h)}}=\frac{\partial L}{\partial \boldsymbol{z}} \boldsymbol{x}^{\top}\]</span></p><p>以上推导过程在<strong>必要的</strong>时候根据两个输入形状，会将其输入进行转置或者互换位置进行相乘(<strong>就是以偏导的维度为准，根据实际的偏导维度调整上述式子中任意两个乘积的位置或者将其更改为其输入的偏置</strong>)</p><h3 id="代码实现-2">代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">backword</span>(<span class="params">self,dLdo,X,h,W1,W2</span>):</span><br><span class="line">    <span class="comment"># 传入损失函数对于输入层的梯度</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># h-&gt;o层</span></span><br><span class="line">    <span class="comment"># 根据梯度自行调节   </span></span><br><span class="line">    W2_grad = nd.dot(h.T,dLdo)</span><br><span class="line"></span><br><span class="line">    dLdh = nd.dot(dLdo,W2.T)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    dLdz = dLdh*sigmoid_prime(h)</span><br><span class="line">        </span><br><span class="line">    X = X.reshape((-<span class="number">1</span>,num_inputs))</span><br><span class="line">    <span class="comment"># 改过</span></span><br><span class="line">    W1_grad = nd.dot(X.T, dLdz)</span><br></pre></td></tr></table></figure><h3 id="损失函数的导数">损失函数的导数</h3><p>上述过程介绍了整个网络中梯度的传递过程，只要将损失函数对于输出层梯度带入即可算出各个可学习参数的梯度，依次进行梯度更新。</p><p>大体上损失函数根据实际问题可以分为两类 - 线性回归 - 分类</p><h4 id="线性回归">线性回归</h4><p>回归（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。线性回归输出是一个连续值，因此适用于回归问题。</p><p>其中回归问题中最常用的损失函数就是L2损失:</p><p><span class="math display">\[\ell=\frac{1}{2}\left(\hat{y}^{(i)}-y^{(i)}\right)^{2}\]</span></p><p>其导数为: <span class="math display">\[\ell&#39;=\left(\hat{y}^{(i)}-y^{(i)}\right)\]</span></p><h4 id="分类问题">分类问题</h4><p>线性回归模型适用于输出为连续值，而分类问题适用于像图像类别这样的离散值。</p><p>对于离散值预测问题，可以使用诸如softmax回归在内的分类模型。和线性回归不同，softmax回归的输出单元从一个变成了多个，且引入了softmax运算使输出更适合离散值的预测和训练。</p><p>为了得到一个模型不同类别的预测结果，会将其设置一个阀值，如选择概率最大的标签。</p><p>例如假设训练数据集中真实的标签为狗，猫或鸡(假设可以用4像素表示出这三种动物)，这些标签分别对应的离散值为<span class="math inline">\(y_1,y_2,y_3\)</span></p><p>通常用离散的数值来表示类别。例如<span class="math inline">\(y_1 = 1,y_2 = 2,y_3 = 3\)</span>.一张图片的标签为1,2,和3这三个数值中的一个。</p><p><strong>虽然我们仍然可以通过回归模型来进行建模，将预测值就近点化到1,2和3这三个离散值之一，但这种连续值到离散值的转换通常会影响到分类质量</strong></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>多层感知机-笔记</title>
      <link href="/2022/05/28/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
      <url>/2022/05/28/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="multilayer-perceptionsmlp">Multilayer perceptions(MLP)</h1><h2 id="单层感知机">单层感知机</h2><p>单层感知机又叫做<strong>稠密层 Dense</strong>(<strong>全连接层</strong>，<strong>线性层</strong>) 主要有两个可学习参数，主要有两个可学习参数:<span class="math inline">\(\mathbf{W} \in \mathbb{R}^{m \times n},\mathbf{b} \in \mathbb{R}^{m}\)</span>,输入数据为<span class="math inline">\(\mathbf{x}\in \mathbb{R}^{n}\)</span> 则其输出为:<span class="math inline">\(\mathbf{y}=\mathbf{W} \mathbf{x}+\mathbf{b} \in \mathbb{R}^{m}\)</span> 我们可以将线性回归和Softmax回归总结如下: - 线性回归可以看做是只有1个输出的全连接层 - Softmax回归可以看做是有<span class="math inline">\(\mathbb{m}\)</span>个输出(<span class="math inline">\(\mathbb{m}\)</span>是分类的类别数)进行Softmax运算</p><h2 id="多层感知机">多层感知机</h2><p>因为单层感知机是一个线性运算，所以简单的单个感知机叠加没法处理非线性问题(如&quot;异或&quot;问题)。 为了引入非线性运算，神经网络在每个层之间增加了一个按元素进行非线性运算的操作，也就是激活函数(Activation Function)</p><h3 id="常见的激活函数">常见的激活函数</h3><h4 id="sigmoid">sigmoid</h4><p><span class="math display">\[\operatorname{sigmoid}(x)=\frac{1}{1+\exp (-x)}\]</span></p><h4 id="relu">ReLU</h4><p><span class="math display">\[\operatorname{ReLU}(x)=\max (x,0)\]</span></p><h3 id="mlp">MLP</h3><p>而所谓的多层感知机就是将多个全连接层堆叠，全连接层之间通过激活函数链接。</p><p>常见的单隐藏层和三层隐藏层的网络结构图如图所示(有颜色的是隐藏层)</p><figure><img src="/2022/05/28/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/2.png" alt="网络结构图"><figcaption>网络结构图</figcaption></figure><p>其中隐藏层个数的选择和隐藏层输出维度的选择都是超参数(需要自己设定)。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络</title>
      <link href="/2022/05/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2022/05/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="neural-networks">Neural Networks</h1><h2 id="背景">背景</h2><h3 id="text-handcrafted-features-rightarrow-text-learned-features"><span class="math inline">\(\text { Handcrafted Features } \rightarrow \text { Learned Features }\)</span></h3><figure><img src="/2022/05/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/1.png" alt="手动提取特征到学习特征"><figcaption>手动提取特征到学习特征</figcaption></figure><p>传统的机器学习通常需要对原始数据进行一些特征工程提取数据特征，在放到机器学习模型中。</p><p>而神经网络(以及所谓的深度学习，深度神经网络)可以看做是将传统的机器学习模型训练流程中的人工提取特征换成一个神经网络来进行特征提取。</p><p>将特征提取和线性(或者softmax)回归一起进行训练，所以相比于人工提取特征来说，数据经过训练后的特征提取器(神经网络)得到的特征更适合线性(或者softmax)回归，所以精度往往更加。</p><p><strong>缺点</strong>:神经网络模型相比于其他传统的及其学习模型来说，对于<strong>数据量</strong>和<strong>计算量</strong>的需求往往大数个<strong>数量级</strong>（1000倍，1w倍甚至10w倍），主要因素在于计算机在进行特征提取的时候没有人这么多的先验知识,所以只能通过大量的数据不断地去做损失迭代更新来学习。 ____</p><h2 id="neural-networks中常见的特征提取模型">Neural Networks中常见的特征提取模型</h2><ul><li>Multilayer perceptions(MLP)</li><li>Convolutional neural networks(CNN)</li><li>Recurrent neural networks(RNN)</li><li>Transsformers(最近兴起的)</li><li>Attention mechanism</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dimensionality reduction</title>
      <link href="/2022/05/23/Dimensionality-reduction/"/>
      <url>/2022/05/23/Dimensionality-reduction/</url>
      
        <content type="html"><![CDATA[<h1 id="降维dimensionality-reduction">降维(Dimensionality reduction)</h1><h2 id="背景">背景</h2><p>在机器学习中，相比于<strong>训练误差</strong>，更加关心<strong>泛化误差</strong>。</p><p>为了解决<strong>过拟合</strong>问题，大致有以下方法 * 扩大数据集 * 正则化hexo * 降维</p><p>而在平常时维度过高可能会造成<strong>维度灾难</strong>，而在没有扩大数据集的办法的 情况下可以选择将其降维来解决。</p><h3 id="维度灾难disadvantages">维度灾难(disadvantages)</h3><h1 id="预备知识">预备知识</h1><p><span class="math display">\[\text{Data:}x=(x_{1},x_{2},...,x_{n})^\top_{n\times P}=\left[\begin{matrix}x^\top_{1} \\x^\top_{2} \\...\\x^\top_{n}\end{matrix}\right]=\left[\begin{matrix} x_{11} &amp; x_{12} &amp; ... &amp; x_{1p} \\ x_{21} &amp; x_{22} &amp; ... &amp; x_{2p}\\ ... &amp; ... &amp; ... &amp; ... \\ x_{n1} &amp; x_{n2} &amp; ... &amp;x_{np}\end{matrix}\right]_{n\times p}\]</span></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>123</title>
      <link href="/2022/05/23/123/"/>
      <url>/2022/05/23/123/</url>
      
        <content type="html"><![CDATA[<p><span class="math display">\[b\]</span></p><p>da</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>小批量随机梯度下降</title>
      <link href="/2022/05/23/%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
      <url>/2022/05/23/%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</url>
      
        <content type="html"><![CDATA[<h1 id="mini-batch-stochastic-gradient-descentsgd">Mini-batch Stochastic gradient descent(SGD)</h1><h2 id="适用范围">适用范围</h2><p>小批量随机梯度下降是整个深度学习里面，目前来说几乎是唯一求解的办法</p><p>在机器学习里面可以解决除开决策树之外所有模型的求解(例如SVM,Logistic Regression)</p><h2 id="算法流程">算法流程</h2><hr><ul><li><span class="math inline">\(\text {Train by mini-batch SGD (by various other ways as well)}\)</span><ul><li><span class="math inline">\(\text { w model param, } b \text { batch size, } \eta_{t} \text { learning rate at time } t\)</span></li><li><span class="math inline">\(\text { Randomly initialize } \mathbf{w}_{1}\)</span></li><li><span class="math inline">\(\text { Repeat } t=1,2, \ldots \text { until converge }\)</span><ul><li><span class="math inline">\(Randomly samples I_{t} \subset\{1, \ldots, n\} with \left|I_{t}\right|=b\)</span></li><li><span class="math inline">\(\text {Update} \mathbf{w}_{t+1}=\mathbf{w}_{t}-\eta_{t} \nabla_{\mathbf{w}_{t}} \ell\left(\mathbf{X}_{I_{t}}, \mathbf{y}_{I_{t}}, \mathbf{w}_{t}\right)\)</span></li></ul></li><li><span class="math inline">\(\text { Pros: solve all objectives in this course except for trees }\)</span></li><li><span class="math inline">\(\text { Cons: sensitive to hyper-parameters } b \text { and } \eta_{t}\)</span></li></ul></li></ul><hr><ul><li><span class="math inline">\(\mathbf{w} \text { 是模型的参数，包括线性模型的 } \mathrm{w} \text { (权重)和 } \mathrm{b} \text { (偏移) }\)</span></li><li><span class="math inline">\(\mathbf{b} \text { 表示批量大小}\)</span></li><li><span class="math inline">\(\eta_{t} \text { 表示在t时刻的学习率}\)</span><ul><li><span class="math inline">\(\text{在1时刻(初始时刻)随机初始化}\mathbf{w}_{1}\)</span></li><li><span class="math inline">\(\text{持续迭代} t=1,2, \ldots\text{直至模型收敛(收敛条件:)}\)</span><ul><li><span class="math inline">\(\text{每次迭代从样本中随机选取} I_{t} \subset\{1, \ldots, n\}个样本，选取样本数目为\left|I_{t}\right|=b(批量大小)\)</span></li><li><span class="math inline">\(\text{更新}\mathbf{w},进入下一次迭代的\mathbf{w}为: \mathbf{w}_{t+1}=\mathbf{w}_{t}-\eta_{t} \nabla_{\mathbf{w}_{t}} \ell\left(\mathbf{X}_{I_{t}}, \mathbf{y}_{I_{t}}, \mathbf{w}_{t}\right)\)</span></li></ul></li></ul></li><li><span class="math inline">\(\text{优点：小批量随机梯度下降可以解决出决策树以外的模型}\)</span></li><li><span class="math inline">\(\text{缺点：超参数}\mathbf{b}和\eta需要自己设置(难调)\)</span> ___</li></ul><h2 id="实现代码">实现代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, nd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="comment"># 生成数据集</span></span><br><span class="line">num_inputs = <span class="number">2</span>   <span class="comment">#数据维度</span></span><br><span class="line">num_examples = <span class="number">1000</span>   <span class="comment"># 样本个数</span></span><br><span class="line">true_w = [<span class="number">2</span>, -<span class="number">3.4</span>]  <span class="comment"># 权重初始值</span></span><br><span class="line">true_b = <span class="number">4.2</span>  <span class="comment"># 偏置初始值</span></span><br><span class="line">features = nd.random.normal(scale = <span class="number">1</span>,shape=(num_examples, num_inputs))</span><br><span class="line">labels = true_w[<span class="number">0</span>]*features[:,<span class="number">0</span>]+true_w[<span class="number">1</span>]*features[:,<span class="number">1</span>]+true_b</span><br><span class="line">labels += nd.random.normal(loc = <span class="number">0</span>,scale = <span class="number">0.01</span>, shape=labels.shape) </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    random.shuffle(indices) <span class="comment"># 打乱标号</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        <span class="comment">#从i到min(i+i+batch_size(一个批量), num_exmaples(不够整分就直接拿去全部))</span></span><br><span class="line">        j = nd.array(indices[i : <span class="built_in">min</span>(i+batch_size, num_examples) ] ) </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">yield</span> features.take(j), labels.take(j) <span class="comment"># take根据索引返回对应元素</span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line">w = nd.random.normal(scale = <span class="number">0.01</span>, shape=(num_inputs,<span class="number">1</span>))<span class="comment"># num_inputs(输出维度),1的维度</span></span><br><span class="line">b = nd.zeros(shape = (<span class="number">1</span>,))<span class="comment"># 1,表示张量</span></span><br><span class="line"><span class="comment"># 开辟梯度空间</span></span><br><span class="line">w.attach_grad()</span><br><span class="line">b.attach_grad()</span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">return</span> (y_hat-y.reshape(y_hat.shape))**<span class="number">2</span>/<span class="number">2</span></span><br><span class="line"><span class="comment"># 优化方法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>):</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param[:] = param - lr*param.grad/batch_size</span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            l = loss(net(X,w,b),y)</span><br><span class="line">        l.backward()</span><br><span class="line">        sgd([w,b],lr,batch_size)</span><br><span class="line">    train_l = loss(net(features, w, b), labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %f&#x27;</span>%(epoch+<span class="number">1</span>,train_l.mean().asnumpy()))</span><br></pre></td></tr></table></figure><h2 id="参考">参考:</h2><ul><li>https://www.bilibili.com/video/BV1234y1m75j</li><li>https://www.bilibili.com/read/cv13723919?from=note</li><li>https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习优化方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/05/17/hello-world/"/>
      <url>/2022/05/17/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
