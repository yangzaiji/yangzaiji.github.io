<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>多层感知机</title>
      <link href="/2022/05/25/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
      <url>/2022/05/25/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="neural-networks">Neural Networks</h1><h2 id="背景">背景</h2>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Dimensionality reduction</title>
      <link href="/2022/05/23/Dimensionality-reduction/"/>
      <url>/2022/05/23/Dimensionality-reduction/</url>
      
        <content type="html"><![CDATA[<h1 id="降维dimensionality-reduction">降维(Dimensionality reduction)</h1><h2 id="背景">背景</h2><p>在机器学习中，相比于<strong>训练误差</strong>，更加关心<strong>泛化误差</strong>。</p><p>为了解决<strong>过拟合</strong>问题，大致有以下方法 * 扩大数据集 * 正则化hexo * 降维</p><p>而在平常时维度过高可能会造成<strong>维度灾难</strong>，而在没有扩大数据集的办法的 情况下可以选择将其降维来解决。</p><h3 id="维度灾难disadvantages">维度灾难(disadvantages)</h3><h1 id="预备知识">预备知识</h1><p><span class="math inline">\(\text{Data:}x=(x_{1},x_{2},...,x_{n})^\top_{N\times P}=\begin{bmatrix} x^\top_{1} \\ x^\top_{2} \\ ...\\ x^\top_{n} \end{bmatrix}=\begin{bmatrix}  x_{11} &amp; x_{12} &amp; ... &amp; x_{1p} \\  x_{21} &amp; x_{22} &amp; ... &amp; x_{2p}\\  ... &amp; ... &amp; ... &amp; ... \\  x_{n1} &amp; x_{n2} &amp; ... &amp;x_{np} \end{bmatrix}\)</span></p><p>$$</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>123</title>
      <link href="/2022/05/23/123/"/>
      <url>/2022/05/23/123/</url>
      
        <content type="html"><![CDATA[<p><span class="math display">\[b\]</span></p><p>da</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>小批量随机梯度下降</title>
      <link href="/2022/05/23/%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
      <url>/2022/05/23/%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</url>
      
        <content type="html"><![CDATA[<h1 id="mini-batch-stochastic-gradient-descentsgd">Mini-batch Stochastic gradient descent(SGD)</h1><h2 id="适用范围">适用范围</h2><p>小批量随机梯度下降是整个深度学习里面，目前来说几乎是唯一求解的办法</p><p>在机器学习里面可以解决除开决策树之外所有模型的求解(例如SVM,Logistic Regression)</p><h2 id="算法流程">算法流程</h2><hr /><ul><li><span class="math inline">\(\text {Train by mini-batch SGD (by various other ways as well)}\)</span><ul><li><span class="math inline">\(\text { w model param, } b \text { batch size, } \eta_{t} \text { learning rate at time } t\)</span></li><li><span class="math inline">\(\text { Randomly initialize } \mathbf{w}_{1}\)</span></li><li><span class="math inline">\(\text { Repeat } t=1,2, \ldots \text { until converge }\)</span><ul><li><span class="math inline">\(Randomly samples I_{t} \subset\{1, \ldots, n\} with \left|I_{t}\right|=b\)</span></li><li><span class="math inline">\(\text {Update} \mathbf{w}_{t+1}=\mathbf{w}_{t}-\eta_{t} \nabla_{\mathbf{w}_{t}} \ell\left(\mathbf{X}_{I_{t}}, \mathbf{y}_{I_{t}}, \mathbf{w}_{t}\right)\)</span></li></ul></li><li><span class="math inline">\(\text { Pros: solve all objectives in this course except for trees }\)</span></li><li><span class="math inline">\(\text { Cons: sensitive to hyper-parameters } b \text { and } \eta_{t}\)</span></li></ul></li></ul><hr /><ul><li><span class="math inline">\(\mathbf{w} \text { 是模型的参数，包括线性模型的 } \mathrm{w} \text { (权重)和 } \mathrm{b} \text { (偏移) }\)</span></li><li><span class="math inline">\(\mathbf{b} \text { 表示批量大小}\)</span></li><li><span class="math inline">\(\eta_{t} \text { 表示在t时刻的学习率}\)</span><ul><li><span class="math inline">\(\text{在1时刻(初始时刻)随机初始化}\mathbf{w}_{1}\)</span></li><li><span class="math inline">\(\text{持续迭代} t=1,2, \ldots\text{直至模型收敛(收敛条件:)}\)</span><ul><li><span class="math inline">\(\text{每次迭代从样本中随机选取} I_{t} \subset\{1, \ldots, n\}个样本，选取样本数目为\left|I_{t}\right|=b(批量大小)\)</span></li><li><span class="math inline">\(\text{更新}\mathbf{w},进入下一次迭代的\mathbf{w}为: \mathbf{w}_{t+1}=\mathbf{w}_{t}-\eta_{t} \nabla_{\mathbf{w}_{t}} \ell\left(\mathbf{X}_{I_{t}}, \mathbf{y}_{I_{t}}, \mathbf{w}_{t}\right)\)</span></li></ul></li></ul></li><li><span class="math inline">\(\text{优点：小批量随机梯度下降可以解决出决策树以外的模型}\)</span></li><li><span class="math inline">\(\text{缺点：超参数}\mathbf{b}和\eta需要自己设置(难调)\)</span> ___</li></ul><h2 id="实现代码">实现代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, nd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="comment"># 生成数据集</span></span><br><span class="line">num_inputs = <span class="number">2</span>   <span class="comment">#数据维度</span></span><br><span class="line">num_examples = <span class="number">1000</span>   <span class="comment"># 样本个数</span></span><br><span class="line">true_w = [<span class="number">2</span>, -<span class="number">3.4</span>]  <span class="comment"># 权重初始值</span></span><br><span class="line">true_b = <span class="number">4.2</span>  <span class="comment"># 偏置初始值</span></span><br><span class="line">features = nd.random.normal(scale = <span class="number">1</span>,shape=(num_examples, num_inputs))</span><br><span class="line">labels = true_w[<span class="number">0</span>]*features[:,<span class="number">0</span>]+true_w[<span class="number">1</span>]*features[:,<span class="number">1</span>]+true_b</span><br><span class="line">labels += nd.random.normal(loc = <span class="number">0</span>,scale = <span class="number">0.01</span>, shape=labels.shape) </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    random.shuffle(indices) <span class="comment"># 打乱标号</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        <span class="comment">#从i到min(i+i+batch_size(一个批量), num_exmaples(不够整分就直接拿去全部))</span></span><br><span class="line">        j = nd.array(indices[i : <span class="built_in">min</span>(i+batch_size, num_examples) ] ) </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">yield</span> features.take(j), labels.take(j) <span class="comment"># take根据索引返回对应元素</span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line">w = nd.random.normal(scale = <span class="number">0.01</span>, shape=(num_inputs,<span class="number">1</span>))<span class="comment"># num_inputs(输出维度),1的维度</span></span><br><span class="line">b = nd.zeros(shape = (<span class="number">1</span>,))<span class="comment"># 1,表示张量</span></span><br><span class="line"><span class="comment"># 开辟梯度空间</span></span><br><span class="line">w.attach_grad()</span><br><span class="line">b.attach_grad()</span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">return</span> (y_hat-y.reshape(y_hat.shape))**<span class="number">2</span>/<span class="number">2</span></span><br><span class="line"><span class="comment"># 优化方法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>):</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param[:] = param - lr*param.grad/batch_size</span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            l = loss(net(X,w,b),y)</span><br><span class="line">        l.backward()</span><br><span class="line">        sgd([w,b],lr,batch_size)</span><br><span class="line">    train_l = loss(net(features, w, b), labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %f&#x27;</span>%(epoch+<span class="number">1</span>,train_l.mean().asnumpy()))</span><br></pre></td></tr></table></figure><h2 id="参考">参考:</h2><ul><li>https://www.bilibili.com/video/BV1234y1m75j</li><li>https://www.bilibili.com/read/cv13723919?from=note</li><li>https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习优化方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/05/17/hello-world/"/>
      <url>/2022/05/17/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
